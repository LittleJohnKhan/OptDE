{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "\n",
    "class ReverseLayerF(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda(GPU)是否可用: False\n",
      "torch的版本: 1.12.1+cu102\n",
      "140221960903744\n",
      "140221960903744\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "torch.manual_seed(seed=20200910)\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.conv1=torch.nn.Sequential(  # 输入torch.Size([64, 1, 28, 28])\n",
    "                torch.nn.Conv2d(1,64,kernel_size=3,stride=1,padding=1),\n",
    "                torch.nn.ReLU(),  # 输出torch.Size([64, 64, 28, 28])\n",
    "                torch.nn.Conv2d(64,128,kernel_size=3,stride=1,padding=1),  # 输出torch.Size([64, 128, 28, 28])\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.MaxPool2d(stride=2,kernel_size=2)  # 输出torch.Size([64, 128, 14, 14])\n",
    "        )\n",
    "\n",
    "        self.dense=torch.nn.Sequential(  # 输入torch.Size([64, 14*14*128])\n",
    "                    torch.nn.Linear(14*14*128,1024),  # 输出torch.Size([64, 1024])\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(p=0.5),\n",
    "                    torch.nn.Linear(1024,10)  # 输出torch.Size([64, 10])        \n",
    "        )\n",
    "        self.layer4cxq1 = torch.nn.Conv2d(2,33,4,4)\n",
    "        self.layer4cxq2 = torch.nn.ReLU()\n",
    "        self.layer4cxq3 = torch.nn.MaxPool2d(stride=2,kernel_size=2)\n",
    "        self.layer4cxq4 = torch.nn.Linear(14*14*128,1024)\n",
    "        self.layer4cxq5 = torch.nn.Dropout(p=0.8)\n",
    "        self.attribute4cxq = nn.Parameter(torch.tensor(20200910.0))\n",
    "        self.attribute4lzq = nn.Parameter(torch.tensor([2.0,3.0,4.0,5.0]))    \n",
    "        self.attribute4hh = nn.Parameter(torch.randn(3,4,5,6))\n",
    "        self.attribute4wyf = nn.Parameter(torch.randn(7,8,9,10))\n",
    "\n",
    "    def forward(self,x):  # torch.Size([64, 1, 28, 28])\n",
    "        x = self.conv1(x)  # 输出torch.Size([64, 128, 14, 14])\n",
    "        x = x.view(-1,14*14*128)  # torch.Size([64, 14*14*128])\n",
    "        x = self.dense(x)  # 输出torch.Size([64, 10])\n",
    "        return x\n",
    "\n",
    "print('cuda(GPU)是否可用:',torch.cuda.is_available())\n",
    "print('torch的版本:',torch.__version__)\n",
    "\n",
    "model = Model() #.cuda()\n",
    "\n",
    "model_1 = model.eval()\n",
    "print(id(model))\n",
    "print(id(model_1))\n",
    "print(id(model)==id(model_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7865, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, f_dims=512):\n",
    "        super(Classifier, self).__init__()\n",
    "        #first fc\n",
    "        self.fc1 = nn.Linear(f_dims, f_dims//4)\n",
    "        self.bn1_fc = nn.BatchNorm1d(f_dims//4)\n",
    "        #second fc\n",
    "        self.fc2 = nn.Linear(f_dims//4, 2) \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1_fc(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "if __name__=='__main__':\n",
    "    classifier = Classifier(f_dims=96)\n",
    "    B=10\n",
    "    input=torch.randn(B,96)\n",
    "    output=classifier(input) #(B,2)\n",
    "    di_s_label=torch.zeros(output.shape[0]).long() #(B,)\n",
    "    crossentropyloss=nn.CrossEntropyLoss()\n",
    "\n",
    "    virtual_di_loss = crossentropyloss(output, di_s_label)\n",
    "    print(virtual_di_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, features,version=0):\n",
    "        self.layer_num = len(features)-1\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.fc_layers = nn.ModuleList([])\n",
    "        for inx in range(self.layer_num):\n",
    "            self.fc_layers.append(nn.Conv1d(features[inx], features[inx+1], kernel_size=1, stride=1))\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                    nn.Linear(features[-1], 128),\n",
    "                    nn.LeakyReLU(negative_slope=0.2),\n",
    "                    nn.Linear(128, 64),\n",
    "                    nn.LeakyReLU(negative_slope=0.2),\n",
    "                    nn.Linear(64, 1),\n",
    "                    nn.Sigmoid())\n",
    "\n",
    "    def forward(self, f):\n",
    "        \n",
    "        feat = f.transpose(1,2)\n",
    "        vertex_num = feat.size(2)\n",
    "\n",
    "        for inx in range(self.layer_num):\n",
    "            feat = self.fc_layers[inx](feat)\n",
    "            feat = self.leaky_relu(feat)\n",
    "        out1 = F.max_pool1d(input=feat, kernel_size=vertex_num).squeeze(-1)\n",
    "        out = self.final_layer(out1) # (B, 1)\n",
    "        return out,out1\n",
    "\n",
    "features=[3, 64,  128, 256, 256, 512] # Features for discriminator\n",
    "D = Discriminator(features=features)\n",
    "pcd= torch.randn(10,2048,3)\n",
    "feat=D(pcd)\n",
    "print(feat[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 3])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "pcd=torch.randn(2048,3)\n",
    "pcd.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140038212241744\n",
      "140038856096560\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.randn(3,3)\n",
    "# b=a\n",
    "b.copy_(a)\n",
    "print(id(a))\n",
    "print(id(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('optde')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3a34733e2c46192af375fb87d5df0a440931a4866d5c12052db3406abc42707"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
